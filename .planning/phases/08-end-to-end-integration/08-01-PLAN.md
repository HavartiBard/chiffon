---
phase: 08-end-to-end-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_full_workflow_e2e.py
  - tests/conftest.py
autonomous: true

must_haves:
  truths:
    - "User submits deployment request through dashboard API, orchestrator decomposes and plans, InfraAgent discovers playbooks, executes them, returns suggestions, all logged to PostgreSQL and git"
    - "Config discovery works: system finds Kuma playbooks in test playbook repo, identifies service metadata, caches results"
    - "Deployment execution completes: InfraAgent runs playbooks via ansible-runner, captures output, updates task status, commits to git audit trail"
    - "Playbook suggestions generated: PlaybookAnalyzer runs ansible-lint on failures, categorizes issues, stores suggestions in database"
    - "Audit trail queryable: PostgreSQL contains task records with status/outcome/resources, git contains .audit/tasks/{task_id}.json commits"
  artifacts:
    - path: "tests/test_full_workflow_e2e.py"
      provides: "Comprehensive E2E test suite covering all 5 success criteria"
      min_lines: 400
    - path: "tests/conftest.py"
      provides: "Shared E2E test fixtures (playbook repos, mock services, database setup)"
      contains: "e2e_test_db"
  key_links:
    - from: "tests/test_full_workflow_e2e.py"
      to: "src/dashboard/api.py"
      via: "TestClient POST to /api/dashboard/chat"
      pattern: "client\\.post.*dashboard/chat"
    - from: "tests/test_full_workflow_e2e.py"
      to: "src/orchestrator/service.py"
      via: "OrchestratorService.submit_request()"
      pattern: "submit_request"
    - from: "tests/test_full_workflow_e2e.py"
      to: "src/agents/infra_agent/agent.py"
      via: "InfraAgent.execute_work()"
      pattern: "execute_work"
    - from: "tests/test_full_workflow_e2e.py"
      to: "src/orchestrator/git_service.py"
      via: "GitService.commit_task_outcome()"
      pattern: "commit_task_outcome"
---

<objective>
Create comprehensive end-to-end integration test suite that validates the complete Chiffon workflow from user request through execution to audit trail.

Purpose: Validates that all Phase 1-7 components integrate correctly and satisfy E2E-01 through E2E-04 requirements. Tests the full workflow with realistic scenarios, ensures auditability and state tracking work as designed.

Output: test_full_workflow_e2e.py with 25+ test methods covering all 5 Phase 8 success criteria, conftest.py fixtures for E2E test infrastructure.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md

# Phase summaries for integration context
@.planning/phases/03-orchestrator-core/03-05-SUMMARY.md
@.planning/phases/05-state-and-audit/05-04-SUMMARY.md
@.planning/phases/06-infrastructure-agent/06-01-SUMMARY.md
@.planning/phases/07-user-interface/07-06-SUMMARY.md

# Existing E2E test patterns
@tests/test_ui_e2e.py
@tests/test_infra_agent_e2e.py
@tests/test_orchestrator_e2e.py

# Key integration points
@src/dashboard/api.py
@src/orchestrator/service.py
@src/agents/infra_agent/agent.py
@src/orchestrator/git_service.py
@src/orchestrator/audit.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create comprehensive E2E test suite</name>
  <files>tests/test_full_workflow_e2e.py</files>
  <action>
Create test_full_workflow_e2e.py with comprehensive integration tests covering all 5 Phase 8 success criteria:

**Test Class Structure:**
1. TestFullUserRequestFlow (E2E-01)
   - test_user_submits_kuma_deployment_request: Submit "Deploy Kuma Uptime to homelab and add our existing portals to the config" via dashboard API
   - test_orchestrator_parses_intent: Verify RequestDecomposer extracts deploy_kuma + add_portals_to_config intents
   - test_orchestrator_generates_multi_step_plan: Verify WorkPlanner creates sequential tasks (deployment, config update)
   - test_plan_requires_user_approval: Verify plan status is pending_approval, not auto-executed
   - test_user_approves_plan: POST to /api/dashboard/approve, verify dispatch_started=True

2. TestConfigDiscovery (E2E-02)
   - test_system_finds_kuma_playbooks: Create temp playbook repo with kuma-deploy.yml and kuma-config-update.yml, verify PlaybookDiscovery finds both
   - test_identifies_service_metadata: Verify PlaybookMetadata includes service="kuma", description, required_vars
   - test_suggests_portals_to_include: Mock existing portal configs (portal-1, portal-2), verify system identifies them for inclusion
   - test_playbook_cache_populated: Verify playbook_cache table contains discovered playbooks

3. TestDeploymentExecution (E2E-03)
   - test_user_approves_then_execution_starts: Approve plan, verify InfraAgent receives WorkRequest via RabbitMQ
   - test_infra_agent_executes_playbooks_in_sequence: Mock ansible-runner, verify kuma-deployment.yml runs before kuma-config-update.yml
   - test_execution_output_streamed: Mock WebSocket broadcast, verify execution updates sent to dashboard
   - test_all_steps_logged_to_postgresql: Query tasks table, verify task records contain status, outcome, resources_used
   - test_execution_handles_failures_gracefully: Mock ansible-runner failure, verify task status=failed, error logged

4. TestPlaybookSuggestions (E2E-04)
   - test_analyzer_runs_after_failure: Mock ansible-runner failure, verify PlaybookAnalyzer.analyze() called
   - test_suggestions_categorized: Verify AnalysisResult contains idempotency/error_handling/etc. categories
   - test_suggestions_stored_in_database: Query playbook_suggestions table, verify suggestion records exist
   - test_user_can_accept_suggestion: Mock acceptance workflow, verify suggestion applied to playbook
   - test_improvement_committed_to_git: Verify git commit for suggestion acceptance

5. TestAuditTrailComplete (E2E-04)
   - test_git_repo_contains_commit: Verify .audit/tasks/{task_id}.json exists in git after task completion
   - test_commit_includes_task_details: Parse audit JSON, verify includes task_id, status, plan_id, execution_result
   - test_postgresql_records_task_state: Query tasks table, verify record includes status, outcome, resources_used, timestamps
   - test_user_reviews_audit_trail_from_ui: GET /api/dashboard/audit/task/{task_id}, verify response includes git commit hash + DB record
   - test_audit_trail_queryable: Query audit.get_failures(days=7), verify failed tasks returned

6. TestFullWorkflowIntegration
   - test_complete_kuma_deployment_workflow: End-to-end happy path (request → plan → approve → execute → suggestions → audit)
   - test_workflow_with_resource_constraints: Mock low agent capacity, verify PauseManager pauses work
   - test_workflow_with_external_ai_fallback: Mock quota <20%, verify ExternalAIFallback routes to Claude
   - test_multiple_agents_tracked_independently: Start 3 desktop agents, verify orchestrator tracks resources for all

**Implementation Notes:**
- Use pytest-asyncio for async test methods
- Use FastAPI TestClient for dashboard API calls
- Mock ansible-runner subprocess calls (avoid actual playbook execution)
- Use in-memory SQLite for test database (pytest fixture)
- Mock RabbitMQ with AsyncMock (no actual message broker in tests)
- Create temp git repo for audit trail tests (cleanup in fixture teardown)
- Parametrize tests across asyncio/trio/curio backends where applicable
- Use existing test patterns from test_ui_e2e.py and test_infra_agent_e2e.py

**Coverage targets:**
- All 5 Phase 8 success criteria exercised
- All 4 E2E requirements validated
- Integration points between phases tested (orchestrator → infra agent, git service → audit queries)
- Error scenarios covered (failures, timeouts, resource constraints)

**Key assertions:**
- Task status transitions: pending → running → completed/failed
- Audit trail completeness: PostgreSQL + git both contain records
- Real-time updates: WebSocket broadcasts execution events
- Playbook suggestions: Analyzer produces actionable recommendations
- Resource awareness: PauseManager respects capacity constraints
  </action>
  <verify>
Run: pytest tests/test_full_workflow_e2e.py -v
Expected: 25+ tests pass, 0 failures
Check coverage: pytest --cov=src.orchestrator --cov=src.agents.infra_agent --cov=src.dashboard tests/test_full_workflow_e2e.py
Expected: >80% coverage of integration paths
  </verify>
  <done>
test_full_workflow_e2e.py exists with 25+ test methods, all tests pass, all 5 success criteria validated via assertions, test execution completes in <10 seconds
  </done>
</task>

<task type="auto">
  <name>Task 2: Create shared E2E test fixtures</name>
  <files>tests/conftest.py</files>
  <action>
Extend tests/conftest.py with shared E2E test fixtures:

**New Fixtures:**

1. e2e_test_db() - PostgreSQL test database
   - Create in-memory SQLite or temp PostgreSQL instance
   - Run all Alembic migrations (001-007)
   - Populate with test data: agent_registry entries, sample playbooks in playbook_cache
   - Yield session
   - Cleanup: drop all tables, close connections

2. temp_git_repo() - Temporary git repository for audit trail tests
   - Create temp directory
   - git init
   - Create .audit/tasks/ directory
   - Yield repo path
   - Cleanup: remove temp directory

3. mock_playbook_repo() - Sample playbook repository
   - Create temp directory with realistic playbook structure:
     - kuma-deploy.yml (valid Galaxy playbook with chiffon metadata)
     - kuma-config-update.yml (config management playbook)
     - postgres-setup.yml (unrelated playbook for discovery tests)
   - Include valid vars, tags, metadata headers
   - Yield repo path
   - Cleanup: remove temp directory

4. mock_ansible_runner() - Mock ansible-runner for execution tests
   - Mock ansible_runner.run() to return ExecutionSummary
   - Parametrize success/failure scenarios
   - Capture calls for assertion
   - Return: (mock_obj, call_history)

5. mock_rabbitmq() - Mock RabbitMQ connection
   - Mock aio-pika Connection, Channel, Queue
   - Track publish calls
   - Simulate message delivery
   - Return: (mock_connection, published_messages)

6. mock_litellm() - Mock LiteLLM service
   - Mock quota queries (return 0.5 = 50% remaining)
   - Mock completion calls (return structured responses)
   - Track usage for cost assertions
   - Return: (mock_client, usage_log)

7. orchestrator_service_e2e() - Fully initialized OrchestratorService
   - Create OrchestratorService with all dependencies
   - Use e2e_test_db for database
   - Use mock_rabbitmq for message bus
   - Use temp_git_repo for audit
   - Yield service instance
   - Cleanup: disconnect service

8. infra_agent_e2e() - Fully initialized InfraAgent
   - Create InfraAgent with mock_playbook_repo
   - Use mock_ansible_runner for execution
   - Yield agent instance

9. dashboard_client_e2e() - Dashboard TestClient
   - Create FastAPI TestClient for dashboard
   - Monkeypatch orchestrator_client to use orchestrator_service_e2e
   - Yield client

**Implementation Notes:**
- Use pytest scope="function" for isolation (each test gets fresh fixtures)
- Use contextlib.asynccontextmanager for async fixtures
- Reuse existing fixtures from conftest.py where possible (test_db, etc.)
- Add cleanup handlers to prevent resource leaks
- Document fixture dependencies in docstrings

**Why shared fixtures:**
- E2E tests require complex setup (DB + git + RabbitMQ + agents)
- Reduce boilerplate in test methods
- Ensure consistent test environments
- Enable parametrization and reusability
  </action>
  <verify>
Run: pytest tests/test_full_workflow_e2e.py --collect-only
Expected: All tests collected successfully, no fixture errors
Run: pytest tests/test_full_workflow_e2e.py -k "test_complete_kuma_deployment_workflow" -v
Expected: Single E2E test passes using all fixtures
  </verify>
  <done>
conftest.py contains 9 new E2E fixtures, all fixtures yield and cleanup correctly, no resource leaks, test_full_workflow_e2e.py uses fixtures successfully
  </done>
</task>

<task type="auto">
  <name>Task 3: Validate E2E requirements coverage</name>
  <files>tests/test_full_workflow_e2e.py</files>
  <action>
Add requirement traceability to test_full_workflow_e2e.py:

**Requirement Markers:**
- Add pytest.mark decorators linking tests to requirements:
  - @pytest.mark.e2e_01 for E2E-01 tests
  - @pytest.mark.e2e_02 for E2E-02 tests
  - @pytest.mark.e2e_03 for E2E-03 tests
  - @pytest.mark.e2e_04 for E2E-04 tests

**Module Docstring:**
Add comprehensive docstring at top of test_full_workflow_e2e.py:
```python
"""End-to-end integration tests for Phase 8: Complete Workflow Validation.

Requirement Coverage:
- E2E-01: Full workflow (user request → orchestrator → agent → git)
  Tests: test_user_submits_kuma_deployment_request, test_orchestrator_parses_intent,
         test_orchestrator_generates_multi_step_plan, test_plan_requires_user_approval,
         test_user_approves_plan
- E2E-02: Config discovery (find Kuma playbooks, identify metadata, suggest portals)
  Tests: test_system_finds_kuma_playbooks, test_identifies_service_metadata,
         test_suggests_portals_to_include, test_playbook_cache_populated
- E2E-03: Deployment execution (agent runs playbooks, streams output, logs to DB)
  Tests: test_user_approves_then_execution_starts, test_infra_agent_executes_playbooks_in_sequence,
         test_execution_output_streamed, test_all_steps_logged_to_postgresql,
         test_execution_handles_failures_gracefully
- E2E-04: Audit trail (suggestions generated, git commits, PostgreSQL records, UI review)
  Tests: test_analyzer_runs_after_failure, test_suggestions_categorized,
         test_suggestions_stored_in_database, test_user_can_accept_suggestion,
         test_improvement_committed_to_git, test_git_repo_contains_commit,
         test_commit_includes_task_details, test_postgresql_records_task_state,
         test_user_reviews_audit_trail_from_ui, test_audit_trail_queryable

Success Criteria Coverage:
1. Full user request flow - tests validate request → plan → approval → dispatch
2. Config discovery - tests validate playbook discovery, metadata extraction, caching
3. Deployment execution - tests validate playbook execution, output capture, logging
4. Playbook suggestions - tests validate analyzer integration, suggestion storage
5. Audit trail complete - tests validate git commits, PostgreSQL records, UI queries

Integration Points Tested:
- Dashboard API → OrchestratorService → InfraAgent
- InfraAgent → PlaybookDiscovery → PlaybookExecutor → PlaybookAnalyzer
- OrchestratorService → GitService → PostgreSQL audit queries
- Dashboard WebSocket → execution monitoring UI
- PauseManager → AgentRegistry capacity queries
- ExternalAIFallback → LiteLLM quota checks
"""
```

**Verification Helper:**
Add utility function at bottom of test file:
```python
def validate_requirement_coverage():
    """Validate that all E2E requirements have test coverage.

    Returns dict with requirement -> test count mapping.
    """
    e2e_01_tests = [name for name in dir() if name.startswith("test_") and "request_flow" in name or "intent" in name or "plan" in name]
    e2e_02_tests = [name for name in dir() if name.startswith("test_") and "discovery" in name or "playbook" in name]
    e2e_03_tests = [name for name in dir() if name.startswith("test_") and "execution" in name or "logged" in name]
    e2e_04_tests = [name for name in dir() if name.startswith("test_") and "audit" in name or "suggestion" in name or "git" in name]

    return {
        "E2E-01": len(e2e_01_tests),
        "E2E-02": len(e2e_02_tests),
        "E2E-03": len(e2e_03_tests),
        "E2E-04": len(e2e_04_tests),
    }
```

**README Section:**
Add section to tests/README.md (create if doesn't exist):
```markdown
## Phase 8: End-to-End Integration Tests

test_full_workflow_e2e.py validates the complete Chiffon workflow:

**Run all E2E tests:**
```bash
pytest tests/test_full_workflow_e2e.py -v
```

**Run by requirement:**
```bash
pytest tests/test_full_workflow_e2e.py -m e2e_01  # Full workflow tests
pytest tests/test_full_workflow_e2e.py -m e2e_02  # Config discovery tests
pytest tests/test_full_workflow_e2e.py -m e2e_03  # Execution tests
pytest tests/test_full_workflow_e2e.py -m e2e_04  # Audit trail tests
```

**Validate coverage:**
```bash
pytest tests/test_full_workflow_e2e.py --cov=src --cov-report=html
```
```

**Implementation Notes:**
- Use descriptive test names that map to requirements
- Add comments in test methods explaining which success criterion they validate
- Include assertions that verify observable behaviors from ROADMAP.md success criteria
- Ensure each requirement has 4+ tests covering happy path + edge cases
  </action>
  <verify>
Run: pytest tests/test_full_workflow_e2e.py -m e2e_01 --collect-only
Expected: 5+ E2E-01 tests collected
Run: pytest tests/test_full_workflow_e2e.py -m e2e_02 --collect-only
Expected: 4+ E2E-02 tests collected
Run: pytest tests/test_full_workflow_e2e.py -m e2e_03 --collect-only
Expected: 5+ E2E-03 tests collected
Run: pytest tests/test_full_workflow_e2e.py -m e2e_04 --collect-only
Expected: 10+ E2E-04 tests collected
Check: grep -c "@pytest.mark.e2e_" tests/test_full_workflow_e2e.py
Expected: 25+ markers
  </verify>
  <done>
All test methods decorated with appropriate requirement markers, module docstring documents coverage, tests/README.md contains E2E test instructions, all 4 requirements have 4+ tests each
  </done>
</task>

</tasks>

<verification>
**Plan-level verification:**

1. Run full E2E test suite:
   ```bash
   pytest tests/test_full_workflow_e2e.py -v --tb=short
   ```
   Expected: 25+ tests pass, <10 second execution, 0 failures

2. Validate requirement coverage:
   ```bash
   pytest tests/test_full_workflow_e2e.py -m e2e_01 -v  # E2E-01: Full workflow
   pytest tests/test_full_workflow_e2e.py -m e2e_02 -v  # E2E-02: Config discovery
   pytest tests/test_full_workflow_e2e.py -m e2e_03 -v  # E2E-03: Deployment execution
   pytest tests/test_full_workflow_e2e.py -m e2e_04 -v  # E2E-04: Audit trail
   ```
   Expected: All requirement-tagged tests pass

3. Check integration coverage:
   ```bash
   pytest tests/test_full_workflow_e2e.py --cov=src.orchestrator --cov=src.agents.infra_agent --cov=src.dashboard --cov-report=term-missing
   ```
   Expected: >80% coverage of orchestrator/infra_agent/dashboard integration paths

4. Validate fixtures work correctly:
   ```bash
   pytest tests/test_full_workflow_e2e.py -k "test_complete_kuma_deployment_workflow" -v -s
   ```
   Expected: Full workflow test passes using all fixtures, no resource leaks

5. Test error scenarios:
   ```bash
   pytest tests/test_full_workflow_e2e.py -k "failure" -v
   ```
   Expected: All failure scenario tests pass (graceful degradation validated)
</verification>

<success_criteria>
**Plan 08-01 is complete when:**

1. test_full_workflow_e2e.py exists with 25+ test methods covering all 5 success criteria
2. All tests pass with 0 failures and <10 second execution time
3. conftest.py contains 9 E2E fixtures that setup/teardown correctly
4. All 4 E2E requirements (E2E-01 through E2E-04) have 4+ tests each
5. Test methods decorated with pytest.mark.e2e_XX requirement markers
6. Module docstring documents requirement coverage and integration points
7. Integration coverage >80% for orchestrator, infra_agent, dashboard modules
8. Error scenarios tested (failures, timeouts, resource constraints)
9. No resource leaks (database connections, temp files, git repos cleaned up)
10. tests/README.md documents how to run E2E tests by requirement
</success_criteria>

<output>
After completion, create `.planning/phases/08-end-to-end-integration/08-01-SUMMARY.md` following the standard summary template.

Include:
- Test count and pass rate
- Requirement coverage mapping (E2E-01 → test names)
- Integration points validated
- Performance metrics (execution time, coverage %)
- Any issues found and resolutions
</output>
