# Phase 6 Plan 06: E2E Integration & Tests

---
phase: 06-infrastructure-agent
plan: 06
type: execute
wave: 3
depends_on: ["06-01", "06-02", "06-03", "06-04", "06-05"]
files_modified:
  - tests/test_infra_agent_e2e.py
  - tests/test_infra_orchestrator_integration.py
  - src/agents/infra_agent/__init__.py
autonomous: true
must_haves:
  truths:
    - "Full workflow tested: discovery -> mapping -> execution -> suggestions"
    - "Integration with orchestrator verified (RabbitMQ work dispatch)"
    - "Multi-scenario coverage: success, failure, no-match, template generation"
    - "All INFRA requirements verified end-to-end"
    - "All Phase 6 unit tests pass together (no conflicts)"
  artifacts:
    - path: "tests/test_infra_agent_e2e.py"
      provides: "End-to-end tests for infrastructure agent"
    - path: "tests/test_infra_orchestrator_integration.py"
      provides: "Integration tests with orchestrator service"
  key_links:
    - from: "tests/test_infra_agent_e2e.py"
      to: "src/agents/infra_agent/agent.py"
      via: "full workflow execution"
      pattern: "InfraAgent.*execute_work"
    - from: "tests/test_infra_orchestrator_integration.py"
      to: "src/orchestrator/service.py"
      via: "work dispatch"
      pattern: "dispatch.*infra"
---

<objective>
Create comprehensive end-to-end integration tests that verify the complete infrastructure agent workflow from discovery through execution and suggestions, including orchestrator integration.

Purpose: This validates that all INFRA requirements are met and all Phase 6 components work together correctly. E2E tests catch integration issues that unit tests miss.

Output: Complete test suite covering full workflows, multi-scenario testing, and orchestrator integration for Phase 6 completion verification.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-infrastructure-agent/06-CONTEXT.md
@.planning/phases/06-infrastructure-agent/06-RESEARCH.md
@src/agents/infra_agent/agent.py
@src/agents/infra_agent/playbook_discovery.py
@src/agents/infra_agent/task_mapper.py
@src/agents/infra_agent/executor.py
@src/agents/infra_agent/analyzer.py
@src/agents/infra_agent/template_generator.py
@src/orchestrator/service.py
@src/common/protocol.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create E2E workflow tests</name>
  <files>
    tests/test_infra_agent_e2e.py
  </files>
  <action>
Create comprehensive end-to-end tests for the infrastructure agent:

1. Create tests/test_infra_agent_e2e.py

2. TestE2EPlaybookDiscovery:
   - test_full_discovery_workflow:
     - Create temp repo with 3 sample playbooks (kuma-deploy.yml, portainer-deploy.yml, invalid.yml)
     - Initialize InfraAgent with repo path
     - Call discover_playbooks()
     - Verify 2 valid playbooks discovered (invalid skipped)
     - Verify metadata extracted (service name, tags)
     - Verify cache populated
     - Call discover_playbooks() again (should use cache)
     - Verify cache hit (no rescan)

   - test_discovery_with_force_refresh:
     - Discover playbooks (populates cache)
     - Add new playbook to temp repo
     - Call discover_playbooks(force_refresh=True)
     - Verify new playbook discovered

3. TestE2ETaskMapping:
   - test_exact_match_workflow:
     - Setup InfraAgent with kuma-deploy.yml
     - Map "Deploy Kuma" -> kuma-deploy.yml
     - Verify confidence = 1.0, method = "exact"

   - test_semantic_match_workflow:
     - Setup InfraAgent with monitoring-deploy.yml
     - Map "Set up service monitoring" -> monitoring-deploy.yml
     - Verify confidence >= 0.85, method = "semantic"
     - Map same intent again -> verify method = "cached"

   - test_no_match_workflow:
     - Setup InfraAgent with unrelated playbooks
     - Map "Deploy NonExistentService"
     - Verify confidence < 0.85, method = "none"
     - Verify suggestion includes template generation option

   - test_multi_match_workflow:
     - Setup with kuma-deploy.yml and kuma-update.yml
     - Map "Kuma operations"
     - Verify alternatives list has both playbooks

4. TestE2EPlaybookExecution:
   - test_successful_execution_workflow (mock ansible-runner):
     - Setup InfraAgent
     - Create WorkRequest with work_type="run_playbook"
     - Execute work
     - Verify WorkResult status="completed"
     - Verify summary includes duration_ms, changed_count

   - test_failed_execution_triggers_analysis (mock ansible-runner + ansible-lint):
     - Setup InfraAgent
     - Mock ansible-runner to return failed status
     - Execute playbook
     - Verify analyzer.analyze_playbook() called
     - Verify suggestions in WorkResult output

   - test_service_level_intent_execution:
     - Setup InfraAgent with kuma-deploy.yml
     - Create WorkRequest with work_type="deploy_service", parameters={"intent": "Deploy Kuma"}
     - Execute work
     - Verify task mapping occurred
     - Verify playbook execution occurred
     - Verify WorkResult contains execution summary

5. TestE2ESuggestions:
   - test_suggestions_categorized:
     - Mock ansible-lint to return mixed rule violations
     - Analyze playbook
     - Verify suggestions grouped by category
     - Verify reasoning present for each suggestion

   - test_suggestions_persisted:
     - With database session
     - Analyze playbook
     - Query playbook_suggestions table
     - Verify suggestions stored

6. TestE2ETemplateGeneration:
   - test_template_generation_workflow:
     - Create WorkRequest with work_type="generate_template", parameters={"service_name": "myapp"}
     - Execute work
     - Verify WorkResult contains playbook_content
     - Verify role_structure present
     - Verify YAML is valid

   - test_template_write_to_disk:
     - Generate template with write_to_disk=True
     - Verify files created in output directory
     - Verify playbook file valid YAML
     - Verify role directory structure

7. Use pytest-asyncio for async tests.

8. Use temporary directories for all file operations.

9. Mock ansible-runner and ansible-lint to avoid external dependencies in CI.
  </action>
  <verify>
    - [ ] All E2E tests pass: `pytest tests/test_infra_agent_e2e.py -v`
    - [ ] Tests cover full workflow: discovery -> mapping -> execution -> suggestions
    - [ ] Tests cover template generation workflow
    - [ ] Tests use mocks for ansible-runner and ansible-lint
  </verify>
  <done>Comprehensive E2E tests created covering all infrastructure agent workflows</done>
</task>

<task type="auto">
  <name>Task 2: Create orchestrator integration tests</name>
  <files>
    tests/test_infra_orchestrator_integration.py
  </files>
  <action>
Create integration tests verifying InfraAgent works with the orchestrator:

1. Create tests/test_infra_orchestrator_integration.py

2. TestOrchestratorInfraAgentIntegration:
   - test_orchestrator_routes_to_infra_agent:
     - Mock OrchestratorService with infra agent registered
     - Submit request "Deploy Kuma to homelab"
     - Verify request routed to infra agent pool
     - Verify WorkRequest has work_type="deploy_service"

   - test_infra_agent_work_result_handled:
     - Mock InfraAgent returning successful WorkResult
     - Verify orchestrator receives result
     - Verify task status updated in database
     - Verify git audit entry created (via GitService)

   - test_infra_agent_failure_logged:
     - Mock InfraAgent returning failed WorkResult
     - Verify orchestrator receives failure
     - Verify task status = "failed"
     - Verify error logged to execution_logs table

3. TestInfraAgentRegistration:
   - test_infra_agent_registers_capabilities:
     - Create InfraAgent
     - Verify get_agent_capabilities() returns correct dict
     - Verify capabilities include: run_playbook, discover_playbooks, generate_template, analyze_playbook

   - test_infra_agent_heartbeat_includes_resources:
     - Start InfraAgent heartbeat
     - Verify heartbeat includes resource metrics
     - Verify agent_type = "infra"

4. TestWorkDispatchFlow:
   - test_work_request_serialization:
     - Create WorkRequest for run_playbook
     - Serialize to MessageEnvelope
     - Deserialize on InfraAgent
     - Verify parameters preserved

   - test_work_result_serialization:
     - Execute work on InfraAgent
     - Serialize WorkResult to MessageEnvelope
     - Verify result can be deserialized
     - Verify ExecutionSummary fields preserved

5. TestInfraAgentErrorScenarios:
   - test_playbook_not_found_error:
     - Send WorkRequest for nonexistent playbook
     - Verify WorkResult has status="failed"
     - Verify error_message includes "not found"

   - test_execution_timeout_error:
     - Mock ansible-runner to timeout
     - Verify WorkResult has status="failed"
     - Verify error_message includes "timeout"

   - test_invalid_work_type_error:
     - Send WorkRequest with work_type="invalid_type"
     - Verify WorkResult has status="failed"
     - Verify error_message includes "Unknown work_type"

6. TestRequirementVerification:
   - test_INFRA_01_task_mapping:
     - Verify "Deploy Kuma" maps to kuma-deploy.yml
     - Verify mapping uses hybrid strategy
     - Document: INFRA-01 satisfied

   - test_INFRA_02_execution_and_output:
     - Execute playbook
     - Verify structured summary returned (not streaming)
     - Verify summary includes status, duration, changed_count
     - Document: INFRA-02 satisfied

   - test_INFRA_03_improvement_suggestions:
     - Trigger failure
     - Verify suggestions generated
     - Verify suggestions categorized
     - Document: INFRA-03 satisfied

   - test_INFRA_04_template_generation:
     - Generate template for "myservice"
     - Verify Galaxy-compliant output
     - Document: INFRA-04 satisfied

7. Use pytest markers for requirement traceability:
   ```python
   @pytest.mark.infra_requirement("INFRA-01")
   def test_INFRA_01_task_mapping():
       ...
   ```
  </action>
  <verify>
    - [ ] All integration tests pass: `pytest tests/test_infra_orchestrator_integration.py -v`
    - [ ] Tests verify orchestrator <-> InfraAgent communication
    - [ ] All INFRA requirements have verification tests
    - [ ] Error scenarios covered
  </verify>
  <done>Orchestrator integration tests created with requirement verification markers</done>
</task>

<task type="auto">
  <name>Task 3: Finalize exports and run full test suite</name>
  <files>
    src/agents/infra_agent/__init__.py
  </files>
  <action>
Finalize module exports and verify all Phase 6 tests pass together:

1. Update src/agents/infra_agent/__init__.py with complete exports:
   ```python
   """Infrastructure Agent for Ansible playbook orchestration.

   Provides:
   - InfraAgent: Main agent class extending BaseAgent
   - PlaybookDiscovery: Lazy playbook scanning with caching
   - TaskMapper: Hybrid task-to-playbook mapping
   - PlaybookExecutor: ansible-runner based execution
   - PlaybookAnalyzer: ansible-lint based suggestions
   - TemplateGenerator: Galaxy-compliant template generation
   """

   from .agent import InfraAgent
   from .playbook_discovery import PlaybookDiscovery, PlaybookMetadata
   from .task_mapper import TaskMapper, MappingResult
   from .executor import PlaybookExecutor, ExecutionSummary
   from .analyzer import PlaybookAnalyzer, Suggestion, AnalysisResult
   from .template_generator import TemplateGenerator, GeneratedTemplate

   __all__ = [
       "InfraAgent",
       "PlaybookDiscovery",
       "PlaybookMetadata",
       "TaskMapper",
       "MappingResult",
       "PlaybookExecutor",
       "ExecutionSummary",
       "PlaybookAnalyzer",
       "Suggestion",
       "AnalysisResult",
       "TemplateGenerator",
       "GeneratedTemplate",
   ]
   ```

2. Run all Phase 6 tests together:
   ```bash
   pytest tests/test_infra_agent_foundation.py \
          tests/test_task_mapper.py \
          tests/test_playbook_executor.py \
          tests/test_playbook_analyzer.py \
          tests/test_template_generator.py \
          tests/test_infra_agent_e2e.py \
          tests/test_infra_orchestrator_integration.py \
          -v --tb=short
   ```

3. Verify no import conflicts or circular dependencies:
   ```python
   python -c "
   from src.agents.infra_agent import (
       InfraAgent, PlaybookDiscovery, TaskMapper,
       PlaybookExecutor, PlaybookAnalyzer, TemplateGenerator
   )
   print('All imports successful')
   "
   ```

4. Generate test coverage report:
   ```bash
   pytest tests/test_infra_*.py --cov=src/agents/infra_agent --cov-report=html
   ```

5. Verify all migrations run cleanly:
   ```bash
   alembic upgrade head
   alembic downgrade base
   alembic upgrade head
   ```

6. Document test results summary:
   - Total test count (target: 150+)
   - Coverage percentage (target: >85%)
   - All INFRA-XX requirements verified
  </action>
  <verify>
    - [ ] All exports in __init__.py work correctly
    - [ ] All Phase 6 tests pass together (no conflicts)
    - [ ] Test coverage > 85% for src/agents/infra_agent/
    - [ ] Migrations run cleanly (up and down)
    - [ ] No circular import issues
  </verify>
  <done>Module exports finalized, all Phase 6 tests passing, coverage verified</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Run all Phase 6 tests: `pytest tests/test_infra_*.py -v`
2. Run with coverage: `pytest tests/test_infra_*.py --cov=src/agents/infra_agent`
3. Verify migrations: `alembic upgrade head && alembic downgrade base && alembic upgrade head`
4. Check imports: `python -c "from src.agents.infra_agent import InfraAgent, PlaybookDiscovery, TaskMapper, PlaybookExecutor, PlaybookAnalyzer, TemplateGenerator"`
</verification>

<success_criteria>
- All E2E tests pass covering full workflow: discovery -> mapping -> execution -> suggestions
- Orchestrator integration tests verify work dispatch and result handling
- All INFRA-XX requirements have explicit verification tests:
  - INFRA-01: Task-to-playbook mapping verified
  - INFRA-02: Execution with structured output verified
  - INFRA-03: Improvement suggestions verified
  - INFRA-04: Template generation verified
- All Phase 6 tests pass together (no conflicts)
- Test coverage > 85% for src/agents/infra_agent/
- Module exports clean (no circular imports)
- Total test count: 150+ across Phase 6
</success_criteria>

<output>
After completion, create `.planning/phases/06-infrastructure-agent/06-06-SUMMARY.md`
</output>
