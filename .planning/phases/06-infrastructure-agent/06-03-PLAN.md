# Phase 6 Plan 03: Playbook Execution & Output

---
phase: 06-infrastructure-agent
plan: 03
type: execute
wave: 2
depends_on: ["06-01", "06-02"]
files_modified:
  - src/agents/infra_agent/executor.py
  - src/agents/infra_agent/agent.py
  - tests/test_playbook_executor.py
autonomous: true
must_haves:
  truths:
    - "PlaybookExecutor runs ansible-playbook via ansible-runner"
    - "Execution produces structured summary (not line-by-line streaming)"
    - "Summary includes status, duration, changed count, failed tasks, key errors"
    - "Variable passing via extravars works correctly"
    - "Execution stops immediately on first failure"
  artifacts:
    - path: "src/agents/infra_agent/executor.py"
      provides: "PlaybookExecutor using ansible-runner"
      exports: ["PlaybookExecutor", "ExecutionSummary"]
  key_links:
    - from: "src/agents/infra_agent/executor.py"
      to: "ansible_runner"
      via: "ansible_runner.run"
      pattern: "ansible_runner\\.run"
    - from: "src/agents/infra_agent/agent.py"
      to: "src/agents/infra_agent/executor.py"
      via: "composition"
      pattern: "PlaybookExecutor"
---

<objective>
Create the PlaybookExecutor service that runs Ansible playbooks via ansible-runner and produces structured summaries rather than streaming line-by-line output.

Purpose: This implements INFRA-02 - the agent must execute playbooks and return results to the orchestrator. Silent execution with event-based summarization reduces RabbitMQ overhead.

Output: PlaybookExecutor that runs playbooks, captures events, and returns structured ExecutionSummary with status, duration, changed items, and error details.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-infrastructure-agent/06-CONTEXT.md
@.planning/phases/06-infrastructure-agent/06-RESEARCH.md
@src/agents/infra_agent/agent.py (from Plan 01)
@src/common/protocol.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PlaybookExecutor service</name>
  <files>
    src/agents/infra_agent/executor.py
  </files>
  <action>
Create the PlaybookExecutor service using ansible-runner:

1. Create src/agents/infra_agent/executor.py

2. ExecutionSummary Pydantic model:
   - status: str ('successful', 'failed', 'timeout', 'cancelled')
   - exit_code: int (ansible-runner return code)
   - duration_ms: int (execution time in milliseconds)
   - changed_count: int (number of changed tasks)
   - ok_count: int (number of successful tasks)
   - failed_count: int (number of failed tasks)
   - skipped_count: int (number of skipped tasks)
   - failed_tasks: list[str] (names of failed tasks)
   - key_errors: list[str] (error messages from failed tasks, max 5)
   - hosts_summary: dict[str, dict] (per-host stats: ok, changed, failures)

3. PlaybookExecutor class:
   - __init__(repo_path: str, artifact_retention: int = 10):
     - self.repo_path = Path(repo_path).expanduser()
     - self.artifact_retention = artifact_retention (for rotate_artifacts)
     - Validate repo_path exists

   - async execute_playbook(
       playbook_path: str,
       extravars: Optional[dict] = None,
       inventory: Optional[str] = None,
       limit: Optional[str] = None,
       tags: Optional[list[str]] = None,
       timeout_seconds: int = 600
     ) -> ExecutionSummary:
     - Resolve playbook_path relative to repo_path if not absolute
     - Validate playbook file exists
     - Build ansible-runner config:
       - private_data_dir: repo_path
       - playbook: playbook_path (relative)
       - extravars: passed dict or {}
       - inventory: inventory or "inventory" (default)
       - limit: passed value or None
       - tags: passed list or None
       - quiet: True (suppress stdout)
       - rotate_artifacts: self.artifact_retention
     - Run with timeout via asyncio.wait_for()
     - Process events into ExecutionSummary
     - Return summary

   - def _run_ansible_sync(config: dict) -> ansible_runner.Runner:
     - Synchronous wrapper for ansible_runner.run()
     - Called via asyncio.to_thread() for non-blocking execution

   - def _process_events(runner: ansible_runner.Runner) -> ExecutionSummary:
     - Initialize counters: changed=0, ok=0, failed=0, skipped=0
     - Initialize lists: failed_tasks=[], key_errors=[]
     - Track start_time, end_time from events
     - Iterate runner.events:
       - "playbook_on_start": record start_time
       - "playbook_on_stats": extract final stats, record end_time
       - "runner_on_ok": increment ok_count; if changed, increment changed_count
       - "runner_on_failed": increment failed_count, capture task name and error msg
       - "runner_on_skipped": increment skipped_count
     - Calculate duration_ms from timestamps
     - Build hosts_summary from runner.stats if available
     - Return ExecutionSummary

   - async _validate_playbook_exists(playbook_path: str) -> Path:
     - Resolve full path
     - If not exists, raise FileNotFoundError with helpful message
     - Return resolved Path

4. Error handling:
   - PlaybookNotFoundError: Custom exception for missing playbook
   - ExecutionTimeoutError: Raised when timeout_seconds exceeded
   - AnsibleRunnerError: Wrapper for ansible-runner internal errors

5. Variable passing methods (from CONTEXT.md):
   - Primary: extravars dict passed to ansible-runner
   - For complex data: Write to temp JSON file, pass via --extra-vars @file.json
   - Method selection based on extravars complexity (nested dicts -> use file)

6. Use asyncio.to_thread() to run ansible_runner.run() without blocking event loop.

7. Cap key_errors at 5 entries to avoid overwhelming error messages.
  </action>
  <verify>
    - [ ] PlaybookExecutor initializes with valid repo_path
    - [ ] execute_playbook returns ExecutionSummary
    - [ ] Event processing extracts correct counts
    - [ ] Timeout handling works (asyncio.wait_for)
    - [ ] Missing playbook raises PlaybookNotFoundError
  </verify>
  <done>PlaybookExecutor service created using ansible-runner with structured summary output</done>
</task>

<task type="auto">
  <name>Task 2: Integrate executor into InfraAgent</name>
  <files>
    src/agents/infra_agent/agent.py
  </files>
  <action>
Update InfraAgent to use PlaybookExecutor for work execution:

1. Update src/agents/infra_agent/agent.py:

2. Add imports:
   - from .executor import PlaybookExecutor, ExecutionSummary
   - from .task_mapper import TaskMapper, MappingResult

3. Update __init__:
   - Initialize self.executor = PlaybookExecutor(repo_path)
   - Initialize self.task_mapper after playbook_discovery is available

4. Implement async execute_work(work_request: WorkRequest) -> WorkResult:
   - Extract work_type and parameters from work_request
   - Handle work_type cases:

   a. work_type == "run_playbook":
      - Get playbook_path from parameters["playbook_path"]
      - Get extravars from parameters.get("extravars", {})
      - Get inventory from parameters.get("inventory")
      - Get timeout from parameters.get("timeout_seconds", 600)
      - Call executor.execute_playbook(...)
      - Convert ExecutionSummary to WorkResult

   b. work_type == "deploy_service" (service-level intent):
      - Get service_intent from parameters["intent"] (e.g., "Deploy Kuma")
      - Call task_mapper.map_task_to_playbook(service_intent)
      - If no match (confidence < 0.85):
        - Return WorkResult with status="failed", error_message=mapping.suggestion
      - Else:
        - Call executor.execute_playbook(mapping.playbook_path, extravars)
        - Convert ExecutionSummary to WorkResult

   c. work_type == "discover_playbooks":
      - Call playbook_discovery.discover_playbooks(force_refresh=parameters.get("force_refresh", False))
      - Return WorkResult with output=JSON list of playbook metadata

   d. Unknown work_type:
      - Return WorkResult with status="failed", error_message=f"Unknown work_type: {work_type}"

5. Helper method _summary_to_result(summary: ExecutionSummary, task_id: UUID) -> WorkResult:
   - Map ExecutionSummary fields to WorkResult:
     - status: "completed" if summary.status == "successful" else "failed"
     - exit_code: summary.exit_code
     - output: JSON string of summary (serialized)
     - error_message: joined key_errors if failed
     - duration_ms: summary.duration_ms
     - resources_used: {"changed_count": summary.changed_count, "hosts": len(summary.hosts_summary)}

6. Error handling in execute_work:
   - Wrap in try/except
   - PlaybookNotFoundError -> WorkResult(status="failed", error_message="Playbook not found: {path}")
   - ExecutionTimeoutError -> WorkResult(status="failed", error_message="Playbook execution timed out")
   - Generic Exception -> Log full traceback, return WorkResult with error
  </action>
  <verify>
    - [ ] InfraAgent.execute_work handles "run_playbook" work_type
    - [ ] InfraAgent.execute_work handles "deploy_service" with task mapping
    - [ ] InfraAgent.execute_work handles "discover_playbooks"
    - [ ] Error handling returns proper WorkResult on failure
    - [ ] ExecutionSummary correctly converted to WorkResult
  </verify>
  <done>InfraAgent execute_work method implemented with PlaybookExecutor integration</done>
</task>

<task type="auto">
  <name>Task 3: Create tests for executor</name>
  <files>
    tests/test_playbook_executor.py
  </files>
  <action>
Create comprehensive tests for PlaybookExecutor and InfraAgent execution:

1. Create tests/test_playbook_executor.py

2. TestExecutionSummary:
   - test_valid_summary: All fields validate
   - test_status_values: Only allowed status values
   - test_key_errors_max_five: Truncated to 5 entries

3. TestPlaybookExecutorInit:
   - test_valid_repo_path: Initializes with existing directory
   - test_invalid_repo_path: Raises error for nonexistent path
   - test_artifact_retention_default: Default is 10

4. TestPlaybookExecutorValidation:
   - test_playbook_not_found: Raises PlaybookNotFoundError
   - test_playbook_relative_path: Resolves relative to repo_path
   - test_playbook_absolute_path: Accepts absolute paths

5. TestPlaybookExecutorExecution (mock ansible_runner):
   - test_successful_execution: Returns status="successful"
   - test_failed_execution: Returns status="failed" with errors
   - test_timeout_execution: Raises ExecutionTimeoutError
   - test_extravars_passed: extravars passed to ansible-runner
   - test_inventory_override: Custom inventory used
   - test_tags_filter: Tags passed to ansible-runner

6. TestEventProcessing (mock ansible_runner.events):
   - test_count_ok_tasks: ok_count correct
   - test_count_changed_tasks: changed_count correct
   - test_count_failed_tasks: failed_count correct
   - test_extract_failed_task_names: failed_tasks list populated
   - test_extract_error_messages: key_errors populated
   - test_calculate_duration: duration_ms from timestamps
   - test_build_hosts_summary: Per-host stats extracted

7. TestInfraAgentExecution (integration):
   - test_run_playbook_work_type: work_type="run_playbook" calls executor
   - test_deploy_service_work_type: work_type="deploy_service" uses task_mapper
   - test_discover_playbooks_work_type: work_type="discover_playbooks" returns catalog
   - test_unknown_work_type: Returns error for unknown work_type
   - test_playbook_not_found_error: Returns WorkResult with error
   - test_execution_timeout_error: Returns WorkResult with timeout error

8. Use pytest fixtures:
   - mock_ansible_runner: Mocked ansible_runner.run() with configurable events
   - sample_events: List of mock ansible events for testing
   - temp_repo_with_playbook: Temporary directory with sample playbook file

9. Mock ansible-runner for all tests (don't require Ansible installed in CI):
   - Use unittest.mock.patch("ansible_runner.run")
   - Return mock Runner object with configurable status, rc, events
  </action>
  <verify>
    - [ ] All tests pass: `pytest tests/test_playbook_executor.py -v`
    - [ ] Test coverage > 90% for executor.py
    - [ ] Tests use mocked ansible-runner (CI-friendly)
    - [ ] Integration tests verify InfraAgent work dispatch
  </verify>
  <done>Comprehensive test suite for PlaybookExecutor with 30+ test cases covering execution scenarios</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Run tests: `pytest tests/test_playbook_executor.py -v`
2. Verify imports: `python -c "from src.agents.infra_agent.executor import PlaybookExecutor, ExecutionSummary; print('Import OK')"`
3. Verify InfraAgent execute_work: `python -c "from src.agents.infra_agent import InfraAgent; print(InfraAgent.execute_work)"`
</verification>

<success_criteria>
- PlaybookExecutor runs playbooks via ansible-runner (not subprocess)
- Structured ExecutionSummary returned (not line-by-line output)
- Summary includes: status, duration_ms, changed_count, failed_tasks, key_errors
- Variable passing via extravars works correctly
- Timeout handling prevents runaway executions
- InfraAgent.execute_work handles run_playbook, deploy_service, discover_playbooks work types
- All tests pass (target: 30+ test cases)
- CI-friendly: tests mock ansible-runner
</success_criteria>

<output>
After completion, create `.planning/phases/06-infrastructure-agent/06-03-SUMMARY.md`
</output>
