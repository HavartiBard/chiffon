# Phase 6 Plan 04: Improvement Suggestions

---
phase: 06-infrastructure-agent
plan: 04
type: execute
wave: 2
depends_on: ["06-01", "06-03"]
files_modified:
  - src/agents/infra_agent/analyzer.py
  - src/agents/infra_agent/agent.py
  - alembic/versions/007_playbook_suggestions.py
  - src/common/models.py
  - tests/test_playbook_analyzer.py
autonomous: true
must_haves:
  truths:
    - "PlaybookAnalyzer runs ansible-lint programmatically"
    - "Suggestions categorized by type: idempotency, error_handling, performance, best_practices, standards"
    - "Analysis triggered only after playbook failure (not on success)"
    - "Suggestions stored in database with playbook path and timestamp"
    - "Suggestions include reasoning, not just rule violations"
  artifacts:
    - path: "src/agents/infra_agent/analyzer.py"
      provides: "PlaybookAnalyzer using ansible-lint"
      exports: ["PlaybookAnalyzer", "Suggestion", "AnalysisResult"]
    - path: "alembic/versions/007_playbook_suggestions.py"
      provides: "Database migration for suggestions storage"
  key_links:
    - from: "src/agents/infra_agent/analyzer.py"
      to: "ansible-lint"
      via: "subprocess with JSON output"
      pattern: "ansible-lint.*--format.*json"
    - from: "src/agents/infra_agent/agent.py"
      to: "src/agents/infra_agent/analyzer.py"
      via: "post-failure analysis call"
      pattern: "analyzer\\.analyze_playbook"
---

<objective>
Create the PlaybookAnalyzer service that runs ansible-lint on playbooks after execution failures and generates categorized improvement suggestions with reasoning.

Purpose: This implements INFRA-03 - the agent must suggest improvements to playbooks. Post-failure analysis focuses suggestions on actual problems rather than generating noise on every execution.

Output: PlaybookAnalyzer that runs ansible-lint, categorizes findings by type (idempotency, error handling, etc.), and stores suggestions with reasoning for audit and learning.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-infrastructure-agent/06-CONTEXT.md
@.planning/phases/06-infrastructure-agent/06-RESEARCH.md
@src/agents/infra_agent/agent.py
@src/agents/infra_agent/executor.py
@src/common/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create database migration for suggestions</name>
  <files>
    alembic/versions/007_playbook_suggestions.py
    src/common/models.py
  </files>
  <action>
Create database schema for storing playbook improvement suggestions:

1. Create alembic/versions/007_playbook_suggestions.py:
   - Table: playbook_suggestions
   - Columns:
     - id: Integer, primary key, autoincrement
     - playbook_path: String(500), not null
     - task_id: UUID, nullable (FK to tasks if from execution)
     - category: String(50), not null ('idempotency', 'error_handling', 'performance', 'best_practices', 'standards')
     - rule_id: String(100), not null (ansible-lint rule ID)
     - message: Text, not null (lint message)
     - reasoning: Text, nullable (why this matters)
     - line_number: Integer, nullable
     - severity: String(20), not null ('error', 'warning', 'info')
     - status: String(20), not null, default='pending' ('pending', 'applied', 'dismissed')
     - created_at: DateTime, not null
     - resolved_at: DateTime, nullable
   - Indexes:
     - idx_suggestions_playbook on playbook_path
     - idx_suggestions_category on category
     - idx_suggestions_status on status

2. Add PlaybookSuggestion ORM model to src/common/models.py:
   - Match migration columns
   - Add relationship to Task (optional, for execution-triggered suggestions)
   - Add __repr__ method
   - Add CATEGORY_CHOICES = ['idempotency', 'error_handling', 'performance', 'best_practices', 'standards']
   - Add SEVERITY_CHOICES = ['error', 'warning', 'info']
   - Add STATUS_CHOICES = ['pending', 'applied', 'dismissed']
  </action>
  <verify>
    - [ ] Migration 007 runs: `alembic upgrade head`
    - [ ] PlaybookSuggestion model validates correctly
    - [ ] Category, severity, status constrained to valid choices
  </verify>
  <done>Database migration for playbook_suggestions created with categorization support</done>
</task>

<task type="auto">
  <name>Task 2: Create PlaybookAnalyzer service</name>
  <files>
    src/agents/infra_agent/analyzer.py
  </files>
  <action>
Create the PlaybookAnalyzer service using ansible-lint:

1. Create src/agents/infra_agent/analyzer.py

2. Suggestion Pydantic model:
   - category: str (one of CATEGORY_CHOICES)
   - rule_id: str (ansible-lint rule ID, e.g., "no-changed-when")
   - message: str (lint message)
   - reasoning: str (human-readable explanation of why this matters)
   - line_number: Optional[int]
   - file_path: str
   - severity: str ('error', 'warning', 'info')

3. AnalysisResult Pydantic model:
   - playbook_path: str
   - total_issues: int
   - suggestions: list[Suggestion]
   - by_category: dict[str, list[Suggestion]] (grouped by category)
   - analyzed_at: datetime

4. PlaybookAnalyzer class:
   - __init__(db_session: Optional[AsyncSession] = None):
     - self.db = db_session (optional, for persistence)
     - self._rule_to_category: dict mapping rule IDs to categories
     - self._rule_to_reasoning: dict mapping rule IDs to reasoning templates

   - async analyze_playbook(playbook_path: str, task_id: Optional[UUID] = None) -> AnalysisResult:
     - Run ansible-lint with JSON output
     - Parse results into Suggestion objects
     - Categorize by rule ID
     - Add reasoning to each suggestion
     - If db_session provided, persist to playbook_suggestions table
     - Return AnalysisResult

   - def _run_ansible_lint(playbook_path: str) -> list[dict]:
     - Run subprocess: ansible-lint --format json --nocolor {playbook_path}
     - Parse JSON output (handle non-zero exit code, lint returns errors on findings)
     - Return list of issue dicts

   - def _categorize_rule(rule_id: str) -> str:
     - Map rule ID to category using lookup table:
       - "no-changed-when", "command-instead-of-module", "risky-shell-pipe" -> "idempotency"
       - "ignore-errors", "no-handler", "fqcn" -> "error_handling"
       - "package-latest", "literal-compare" -> "performance"
       - "yaml", "name", "syntax-check", "jinja" -> "best_practices"
       - Everything else -> "standards"

   - def _generate_reasoning(rule_id: str, message: str) -> str:
     - Map rule ID to reasoning template:
       - "no-changed-when": "Tasks without changed_when are not idempotent. Running this playbook twice may report false changes."
       - "command-instead-of-module": "Shell/command modules bypass Ansible's idempotency. Use built-in modules (apt, systemd, template) when available."
       - "ignore-errors": "Ignoring errors hides failures. Use failed_when for specific conditions or add error handling."
       - "no-handler": "Handlers ensure services restart only when config changes. Direct notify is more reliable than inline restarts."
       - Default: f"Ansible best practice violation: {message}"

   - def _map_severity(lint_severity: str) -> str:
     - "error" -> "error"
     - "warning" -> "warning"
     - "info", "hint" -> "info"

   - async _persist_suggestions(suggestions: list[Suggestion], playbook_path: str, task_id: Optional[UUID]):
     - For each suggestion, create PlaybookSuggestion record
     - Bulk insert to database
     - Skip duplicates (same playbook_path + rule_id + line_number)

5. Rule-to-category mapping (comprehensive list from ansible-lint docs):
   ```python
   RULE_CATEGORIES = {
       # Idempotency
       "no-changed-when": "idempotency",
       "command-instead-of-module": "idempotency",
       "command-instead-of-shell": "idempotency",
       "risky-shell-pipe": "idempotency",
       "no-free-form": "idempotency",

       # Error handling
       "ignore-errors": "error_handling",
       "no-handler": "error_handling",
       "fqcn": "error_handling",
       "no-log-password": "error_handling",

       # Performance
       "package-latest": "performance",
       "literal-compare": "performance",
       "no-jinja-when": "performance",

       # Best practices
       "yaml": "best_practices",
       "name": "best_practices",
       "syntax-check": "best_practices",
       "jinja": "best_practices",
       "key-order": "best_practices",
       "no-tabs": "best_practices",
   }
   ```

6. Handle edge cases:
   - ansible-lint not installed: Log warning, return empty AnalysisResult
   - YAML parse errors in lint output: Log and skip malformed entries
   - Very large playbooks (>100 issues): Truncate to top 50 by severity
  </action>
  <verify>
    - [ ] PlaybookAnalyzer initializes successfully
    - [ ] analyze_playbook returns AnalysisResult with categorized suggestions
    - [ ] Rule categorization maps correctly
    - [ ] Reasoning generated for common rules
    - [ ] Results persisted to database when db_session provided
  </verify>
  <done>PlaybookAnalyzer service created with ansible-lint integration and categorized suggestions</done>
</task>

<task type="auto">
  <name>Task 3: Integrate analyzer into InfraAgent and create tests</name>
  <files>
    src/agents/infra_agent/agent.py
    tests/test_playbook_analyzer.py
  </files>
  <action>
Update InfraAgent to trigger analysis on failures and create comprehensive tests:

1. Update src/agents/infra_agent/agent.py:

   a. Add imports:
      - from .analyzer import PlaybookAnalyzer, AnalysisResult

   b. Update __init__:
      - Initialize self.analyzer = PlaybookAnalyzer(db_session) if db_session provided

   c. Update execute_work for work_type == "run_playbook" and "deploy_service":
      - After execution completes:
      - If summary.status == "failed":
        - Call analyzer.analyze_playbook(playbook_path, task_id=work_request.task_id)
        - Include analysis_result in WorkResult output
        - Log suggestion count at INFO level

   d. Add new work_type == "analyze_playbook":
      - Get playbook_path from parameters
      - Call analyzer.analyze_playbook(playbook_path)
      - Return WorkResult with analysis_result as JSON output

2. Create tests/test_playbook_analyzer.py:

   TestSuggestionModel:
   - test_valid_suggestion: All fields validate
   - test_category_choices: Only valid categories accepted
   - test_severity_choices: Only valid severities accepted

   TestAnalysisResult:
   - test_by_category_grouping: Suggestions grouped correctly
   - test_total_issues_count: Matches suggestions length

   TestRuleCategorization:
   - test_idempotency_rules: no-changed-when, command-instead-of-module
   - test_error_handling_rules: ignore-errors, no-handler
   - test_performance_rules: package-latest
   - test_best_practices_rules: yaml, name
   - test_unknown_rule_defaults_to_standards

   TestReasoningGeneration:
   - test_no_changed_when_reasoning: Contains "idempotent"
   - test_command_instead_of_module_reasoning: Contains "built-in modules"
   - test_ignore_errors_reasoning: Contains "error handling"
   - test_default_reasoning: Uses generic template

   TestAnsibleLintExecution (mock subprocess):
   - test_lint_successful_parse: JSON output parsed correctly
   - test_lint_with_findings: Non-zero exit code handled
   - test_lint_not_installed: Returns empty result with warning
   - test_lint_timeout: Handles timeout gracefully

   TestPlaybookAnalyzerIntegration (mock subprocess):
   - test_analyze_playbook_returns_result: Full workflow
   - test_suggestions_persisted_to_db: Database insert verified
   - test_duplicate_suggestions_skipped: Idempotent insert
   - test_large_result_truncated: >100 issues truncated to 50

   TestInfraAgentAnalyzerIntegration:
   - test_analysis_triggered_on_failure: Analyzer called when execution fails
   - test_analysis_not_triggered_on_success: Analyzer not called on success
   - test_analyze_playbook_work_type: Direct analysis work type works

3. Use pytest fixtures:
   - mock_ansible_lint_output: Sample JSON output from ansible-lint
   - mock_subprocess: Patched subprocess.run for ansible-lint
   - sample_playbook_path: Temporary playbook file for testing
  </action>
  <verify>
    - [ ] All tests pass: `pytest tests/test_playbook_analyzer.py -v`
    - [ ] Test coverage > 90% for analyzer.py
    - [ ] InfraAgent triggers analysis only on failure
    - [ ] Tests mock ansible-lint subprocess (CI-friendly)
  </verify>
  <done>PlaybookAnalyzer integrated into InfraAgent with post-failure triggering; comprehensive test suite created</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Run migration: `alembic upgrade head`
2. Run tests: `pytest tests/test_playbook_analyzer.py -v`
3. Verify imports: `python -c "from src.agents.infra_agent.analyzer import PlaybookAnalyzer, AnalysisResult; print('Import OK')"`
4. Verify categories: `python -c "from src.agents.infra_agent.analyzer import PlaybookAnalyzer; a = PlaybookAnalyzer(); print(a._rule_to_category)"`
</verification>

<success_criteria>
- PlaybookAnalyzer runs ansible-lint programmatically with JSON output
- Suggestions categorized: idempotency, error_handling, performance, best_practices, standards
- Reasoning provided for each suggestion (not just rule violations)
- Analysis triggered only after playbook failure (not on success)
- Suggestions stored in database with playbook path, category, and timestamp
- InfraAgent execute_work includes analysis in failure response
- All tests pass (target: 25+ test cases)
- CI-friendly: tests mock subprocess
</success_criteria>

<output>
After completion, create `.planning/phases/06-infrastructure-agent/06-04-SUMMARY.md`
</output>
